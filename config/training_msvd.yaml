# config/training_msvd.yaml

project_name: "temporal-moe-vit"
run_name: "msvd_caption_task_final_run"

model:
  embed_dim: 768
  num_layers: 12
  num_heads: 12
  video_patch_size: 16
  # The dataloader will determine the vocabulary size of the *first words* of captions.
  num_answer_classes: 2000 # Placeholder, will be updated.
  text_seq_len: 32         # Max caption length for this dataset is ~20 words.
  frames_per_video: 16
  max_seq_len: 256         # Placeholder, will be auto-calculated.

  moe:
    num_experts: 8
    top_k: 2
    experts:
      motion: { flow_dim: 64 }
      fast_change: { delta_dim: 32 }
      texture: { channels: 3 }

data:
  dataset_name: "msvd-caption"
  # <<<<< MOST IMPORTANT: MAKE SURE THIS PATH IS CORRECT >>>>>
  data_root: "D:/Karm/Projects/datasets/msvd_qa"
  batch_size: 16
  num_workers: 8 
  text_tokenizer: "bert-base-uncased"
  # NEW: Defines how to split the data for a proper experiment
  train_val_test_split: [0.7, 0.1, 0.2] 

training:
  epochs: 25 # A few more epochs for this task
  optimizer: "AdamW"
  learning_rate: 3.0e-5 # A lower, stable learning rate for real data.
  weight_decay: 0.05
  loss_alpha: 0.01
  device: "cuda"

logging:
  checkpoint_dir: "checkpoints/"
  checkpoint_prefix: "msvd_caption_run"